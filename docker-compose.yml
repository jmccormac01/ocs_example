version: "3.7"

services:
    db:
        #image: postgres:10.4-alpine
        #image: postgres:13.3-alpine
        image: postgis/postgis:13-master
        environment:
            - POSTGRES_USER=postgres
            - POSTGRES_MULTIPLE_DATABASES=configdb,observationportal,downtime,sciencearchive
            - POSTGRES_PASSWORD=postgrespass
        volumes:
            - ./docker-postgresql-multiple-databases:/docker-entrypoint-initdb.d
        mem_limit: "2048m"
        healthcheck:
            test: ["CMD-SHELL", "pg_isready -U postgres"]
            interval: 10s
            timeout: 5s
            retries: 5
        restart: always

    configdb:
        image: observatorycontrolsystem/configdb:2.1.1
        ports:
            - "7000:7000"
        environment:
            - DB_HOST=db
            - DB_NAME=configdb
            - DB_USER=postgres
            - DB_PASS=postgrespass
            - SECRET_KEY=ocs_example_configdb_secret_key
            - DEBUG=true
            - OAUTH_CLIENT_ID=configdb_application_client_id
            - OAUTH_CLIENT_SECRET=configdb_application_client_secret
            - OAUTH_TOKEN_URL=http://observation-portal:8000/o/token/
        mem_limit: "512m"
        restart: always
        healthcheck:
            test: ["CMD-SHELL", "wget localhost:7000/genericmodes/ -q -O - > /dev/null 2>&1"]
            interval: 10s
            timeout: 5s
            retries: 5
        command: >
            bash -c "python manage.py migrate 
            && python manage.py init_e2e_data -s ogg --latitude 20.707 --longitude -156.258 --instrument-state=SCHEDULABLE
            && python manage.py runserver 0.0.0.0:7000"
        depends_on:
            db:
                condition: service_healthy

    downtime:
        image: observatorycontrolsystem/downtime:2.3.2
        ports:
            - "7500:7500"
        environment:
            - DB_ENGINE=django.db.backends.postgresql
            - DB_HOST=db
            - DB_NAME=downtime
            - DB_USER=postgres
            - DB_PASS=
            - SECRET_KEY=ocs_example_downtime_secret_key
            - CONFIGDB_URL=http://configdb:7000
            - OAUTH_CLIENT_ID=downtime_application_client_id
            - OAUTH_CLIENT_SECRET=downtime_application_client_secret
            - OAUTH_TOKEN_URL=http://observation-portal:8000/o/token/
            - OAUTH_PROFILE_URL=http://observation-portal:8000/api/profile/
        mem_limit: "512m"
        restart: always
        command: >
            sh -c "python manage.py migrate
            && python manage.py shell -c \"from django.contrib.auth.models import User; User.objects.create_superuser('test_user', 'test_user@fake.com', 'test_pass')\"
            && python manage.py create_downtime -s ogg -e doma -t 1m0a -r Weather --offset-hours=24 --duration-hours=24
            && python manage.py create_downtime -s ogg -e clma -t 2m0a -r Maintenance --offset-hours=-48 --duration-hours=24
            && python manage.py runserver 0.0.0.0:7500"
        depends_on:
            db:
                condition: service_healthy
            configdb:
                condition: service_healthy

    observation-portal:
        image: observatorycontrolsystem/observation-portal:3.1.13
        ports:
            - "8000:8000"
        environment:
            - DB_HOST=db
            - DB_NAME=observationportal
            - DB_USER=postgres
            - DB_PASSWORD=postgrespass
            - DEBUG=true
            - SECRET_KEY=ocs_example_obs_portal_secret_key
            - CONFIGDB_URL=http://configdb:7000
            - DOWNTIMEDB_URL=http://downtime:7500
            - ELASTICSEARCH_URL=
            - CORS_ALLOW_CREDENTIALS=true
            - CORS_ORIGIN_WHITELIST=http://127.0.0.1:8080,http://localhost:8080
            - CSRF_TRUSTED_ORIGINS=127.0.0.1:8080,localhost:8080
        mem_limit: "512m"
        restart: always
        command: >
            sh -c "python manage.py migrate 
            && python manage.py create_user -u test_user -p test_pass --superuser --token=sutoken1234abcd
            && python manage.py create_application -u test_user -n ConfigDB --client-id configdb_application_client_id --client-secret configdb_application_client_secret --redirect-uris http://127.0.0.1:7000
            && python manage.py create_application -u test_user -n Downtime --client-id downtime_application_client_id --client-secret downtime_application_client_secret --redirect-uris http://127.0.0.1:7500/
            && python manage.py create_semester --id TestSemester
            && python manage.py create_proposal --id TestProposal --active --direct --pi test_user --time-allocation
            && python manage.py create_example_requests -p TestProposal -s test_user
            && python manage.py collectstatic --no-input
            && python manage.py runserver 0.0.0.0:8000"
        depends_on:
            db:
                condition: service_healthy
            configdb:
                condition: service_healthy

    adaptive_scheduler:
        image: observatorycontrolsystem/adaptive_scheduler:1.1.10
        restart: always
        links:
            -  redis:redis
        environment:
            -  OPENTSDB_HOSTNAME=opentsdb-path
            -  OPENTSDB_PYTHON_METRICS_TEST_MODE=True
            -  OBSERVATION_PORTAL_URL=http://observation-portal:8000
            -  OBSERVATION_PORTAL_API_TOKEN=sutoken1234abcd
            -  CONFIGDB_URL=http://configdb:7000
            -  DOWNTIMEDB_URL=http://downtime:7500/
            -  REDIS_URL=redis
            -  TELESCOPE_CLASSES=1m0,2m0
            -  SAVE_PICKLE_INPUT_FILES=False
            -  SAVE_JSON_OUTPUT_FILES=False
            -  TIME_BETWEEN_RUNS=300
            -  KERNEL_TIMELIMIT=1200
            -  MODEL_HORIZON=2
            -  MODEL_SLICESIZE=300
            -  NO_WEATHER=True
            -  KERNEL_ALGORITHM=SCIP
        volumes:
            # Edit these volume maps to wherever you want to store the log files and input/output data sets
            -  ./data/input:/data/adaptive_scheduler/input_states/
            -  ./data/output:/data/adaptive_scheduler/output_schedule/
            -  ./data/:/data/adaptive_scheduler/
            -  ./logs/:/ocs/adaptive_scheduler/logs/
        working_dir: /ocs/adaptive_scheduler/
        command: python as.py
        depends_on:
            db:
                condition: service_healthy
            configdb:
                condition: service_healthy
            redis:
                condition: service_healthy

    ocs_frontend:
        image: observatorycontrolsystem/ocs-example-frontend:0.1.0
        ports:
            - "8080:8080"
        restart: always
        environment:
            -  VUE_APP_OBSERVATION_PORTAL_API_URL=http://127.0.0.1:8000
            -  INTERNAL_OBSERVATION_PORTAL_API_URL=http://127.0.0.1:8000
        entrypoint: /entrypoint.sh


    minio1:
        image: minio/minio:RELEASE.2021-06-17T00-10-46Z
        hostname: minio1
        volumes:
            - data1-1:/data1
            - data1-2:/data2
        expose:
            - "9000:9000"
        environment:
            MINIO_ROOT_USER: minio
            MINIO_ROOT_PASSWORD: minio123
            MINIO_REGION_NAME: minioregion
        command: server http://minio{1...4}/data{1...2}
        healthcheck:
            test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
            interval: 30s
            timeout: 20s
            retries: 3
    minio2:
        image: minio/minio:RELEASE.2021-06-17T00-10-46Z
        hostname: minio2
        volumes:
            - data2-1:/data1
            - data2-2:/data2
        expose:
            - "9000"
        environment:
            MINIO_ROOT_USER: minio
            MINIO_ROOT_PASSWORD: minio123
            MINIO_REGION_NAME: minioregion
        command: server http://minio{1...4}/data{1...2}
        healthcheck:
            test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
            interval: 30s
            timeout: 20s
            retries: 3
    minio3:
        image: minio/minio:RELEASE.2021-06-17T00-10-46Z
        hostname: minio3
        volumes:
            - data3-1:/data1
            - data3-2:/data2
        expose:
            - "9000"
        environment:
            MINIO_ROOT_USER: minio
            MINIO_ROOT_PASSWORD: minio123
            MINIO_REGION_NAME: minioregion
        command: server http://minio{1...4}/data{1...2}
        healthcheck:
            test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
            interval: 30s
            timeout: 20s
            retries: 3
    minio4:
        image: minio/minio:RELEASE.2021-06-17T00-10-46Z
        hostname: minio4
        volumes:
            - data4-1:/data1
            - data4-2:/data2
        expose:
            - "9000"
        environment:
            MINIO_ROOT_USER: minio
            MINIO_ROOT_PASSWORD: minio123
            MINIO_REGION_NAME: minioregion
        command: server http://minio{1...4}/data{1...2}
        healthcheck:
            test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
            interval: 30s
            timeout: 20s
            retries: 3

    nginx:
        image: nginx:1.19.2-alpine
        hostname: nginx
        volumes:
        - ./nginx.conf:/etc/nginx/nginx.conf:ro
        ports:
        - "9000:9000"
        depends_on:
        - minio1
        - minio2
        - minio3
        - minio4

    science_archive:
        #image: observatorycontrolsystem/science-archive:1.77.1
        image: sciencearchive-local
        container_name: sciencearchive
        ports:
            - "9500:9500"
        environment:
            - DEBUG=True  # must be true to load static files
            - DB_HOST=db
            - DB_HOST_READER=db
            - DB_NAME=sciencearchive
            - DB_USER=postgres
            - DB_PASS=postgrespass
            - SECRET_KEY=ocs_example_science_archive_secret_key
            - AWS_ACCESS_KEY_ID=minio
            - AWS_SECRET_ACCESS_KEY=minio123
            - AWS_DEFAULT_REGION=minioregion
            - AWS_BUCKET=ocs-example-bucket
            - S3_ENDPOINT_URL=http://minio1:9000
            - OAUTH_CLIENT_ID=science_archive_application_client_id
            - OAUTH_CLIENT_SECRET=science_archive_application_client_secret
            - OAUTH_TOKEN_URL=http://observation-portal:8000/o/token/
            - OAUTH_PROFILE_URL=http://observation-portal:8000/api/profile/
            - PROCESSED_EXCHANGE_ENABLED=False
            # Additional variables for the ingester:
            - API_ROOT=http://localhost:9500/
            - BUCKET=ocs-example-bucket
            - OPENTSDB_PYTHON_METRICS_TEST_MODE=True
        mem_limit: "512m"
        restart: always
        volumes: 
            - ./fits:/fits
            - ./localingester:/localingester
        healthcheck:
            test: ["CMD-SHELL", "wget localhost:9500/ -q -O - > /dev/null 2>&1"]
            interval: 10s
            timeout: 5s
            retries: 5
        command: >
            sh -c "python manage.py migrate ;
            python manage.py collectstatic --no-input ;
            python manage.py runserver 0.0.0.0:9500 &
            pip install -e /localingester/ingester ;
            export AUTH_TOKEN=$$(echo 'from django.contrib.auth.models import User; User.objects.create_superuser(\"test_archive_user\", \"archiveadmin@example.com\", \"test_archive_pass\"); print(User.objects.first().auth_token)' | python manage.py shell) ;
            echo $$AUTH_TOKEN;
            python /fits/init_ingester_routine.py ;
            wait %1
            "
        depends_on:
            minio1:
                condition: service_healthy


    redis:
        image: redis:3.2
        command: ["redis-server", "--appendonly", "yes"]
        restart: always
        ports:
            -  6373:6379
        volumes:
            -  ./data/redis:/data
        healthcheck:
            test: ["CMD", "redis-cli", "ping"]
            interval: 10s
            timeout: 5s
            retries: 30

## By default this config uses default local driver,
## For custom volumes replace with volume driver configuration.
volumes:
    data1-1:
    data1-2:
    data2-1:
    data2-2:
    data3-1:
    data3-2:
    data4-1:
    data4-2:
