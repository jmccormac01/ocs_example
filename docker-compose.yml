version: "3.7"

services:
    db:
        image: postgis/postgis:13-master
        environment:
            - POSTGRES_USER=postgres
            - POSTGRES_MULTIPLE_DATABASES=configdb,observationportal,downtime,sciencearchive
            - POSTGRES_PASSWORD=postgrespass
        volumes:
            - ./docker-postgresql-multiple-databases:/docker-entrypoint-initdb.d
        mem_limit: "2048m"
        healthcheck:
            test: ["CMD-SHELL", "pg_isready -U postgres"]
            interval: 10s
            timeout: 5s
            retries: 5
        restart: always

    configdb:
        image: observatorycontrolsystem/configdb:2.1.1
        ports:
            - "7000:7000"
        environment:
            - DB_HOST=db
            - DB_NAME=configdb
            - DB_USER=postgres
            - DB_PASS=postgrespass
            - SECRET_KEY=ocs_example_configdb_secret_key
            - DEBUG=true
            - OAUTH_CLIENT_ID=configdb_application_client_id
            - OAUTH_CLIENT_SECRET=configdb_application_client_secret
            - OAUTH_TOKEN_URL=http://observation-portal:8000/o/token/
        mem_limit: "512m"
        restart: always
        healthcheck:
            test: ["CMD-SHELL", "wget localhost:7000/genericmodes/ -q -O - > /dev/null 2>&1"]
            interval: 10s
            timeout: 5s
            retries: 5
        command: >
            bash -c "python manage.py migrate 
            && python manage.py init_e2e_data -s ogg --latitude 20.707 --longitude -156.258 --instrument-state=SCHEDULABLE
            && python manage.py runserver 0.0.0.0:7000"
        depends_on:
            db:
                condition: service_healthy

    downtime:
        image: observatorycontrolsystem/downtime:2.3.2
        logging: 
            driver: none
        ports:
            - "7500:7500"
        environment:
            - DB_ENGINE=django.db.backends.postgresql
            - DB_HOST=db
            - DB_NAME=downtime
            - DB_USER=postgres
            - DB_PASS=postgrespass
            - SECRET_KEY=ocs_example_downtime_secret_key
            - CONFIGDB_URL=http://configdb:7000
            - OAUTH_CLIENT_ID=downtime_application_client_id
            - OAUTH_CLIENT_SECRET=downtime_application_client_secret
            - OAUTH_TOKEN_URL=http://observation-portal:8000/o/token/
            - OAUTH_PROFILE_URL=http://observation-portal:8000/api/profile/
        mem_limit: "512m"
        restart: always
        command: >
            sh -c "python manage.py migrate
            && python manage.py shell -c \"from django.contrib.auth.models import User; User.objects.create_superuser('test_user', 'test_user@fake.com', 'test_pass')\"
            && python manage.py create_downtime -s ogg -e doma -t 1m0a -r Weather --offset-hours=24 --duration-hours=24
            && python manage.py create_downtime -s ogg -e clma -t 2m0a -r Maintenance --offset-hours=-48 --duration-hours=24
            && python manage.py runserver 0.0.0.0:7500"
        depends_on:
            db:
                condition: service_healthy
            configdb:
                condition: service_healthy

    observation-portal:
        image: observatorycontrolsystem/observation-portal:3.1.11
        ports:
            - "8000:8000"
        environment:
            - DB_HOST=db
            - DB_NAME=observationportal
            - DB_USER=postgres
            - DB_PASSWORD=postgrespass
            - DEBUG=true
            - SECRET_KEY=ocs_example_obs_portal_secret_key
            - CONFIGDB_URL=http://configdb:7000
            - DOWNTIMEDB_URL=http://downtime:7500
            - ELASTICSEARCH_URL=
            - CORS_ALLOW_CREDENTIALS=true
            - CORS_ORIGIN_WHITELIST=http://127.0.0.1:8080,http://localhost:8080
            - CSRF_TRUSTED_ORIGINS=127.0.0.1:8080,localhost:8080
        mem_limit: "512m"
        restart: always
        command: >
            sh -c "python manage.py migrate 
            && python manage.py create_user -u test_user -p test_pass --superuser --token=sutoken1234abcd
            && python manage.py create_application -u test_user -n ConfigDB --client-id configdb_application_client_id --client-secret configdb_application_client_secret --redirect-uris http://127.0.0.1:7000
            && python manage.py create_application -u test_user -n Downtime --client-id downtime_application_client_id --client-secret downtime_application_client_secret --redirect-uris http://127.0.0.1:7500/
            && python manage.py create_application -u test_user -n Archive --client-id science_archive_application_client_id --client-secret science_archive_application_client_secret --redirect-uris http://127.0.0.1:9500/
            && python manage.py create_semester --id TestSemester
            && python manage.py create_proposal --id TestProposal --active --direct --pi test_user --time-allocation
            && python manage.py create_example_requests -p TestProposal -s test_user
            && python manage.py collectstatic --no-input
            && python manage.py runserver 0.0.0.0:8000"
        depends_on:
            db:
                condition: service_healthy
            configdb:
                condition: service_healthy

    adaptive_scheduler:
        image: observatorycontrolsystem/adaptive_scheduler:1.1.2
        logging: 
            driver: none
        restart: always
        links:
            -  redis:redis
        environment:
            -  OPENTSDB_HOSTNAME=opentsdb-path
            -  OPENTSDB_PYTHON_METRICS_TEST_MODE=True
            -  OBSERVATION_PORTAL_URL=http://observation-portal:8000
            -  CONFIGDB_URL=http://configdb:7000
            -  DOWNTIMEDB_URL=http://downtime:7500/
            -  ELASTICSEARCH_URL=http://elasticsearch-path:9200
            -  REDIS_URL=redis
            -  SCHEDULER_SLEEP=30
            -  SCHEDULER_TIMELIMIT=1200
            -  SCHEDULER_HORIZON=2
            -  SCHEDULER_SLICESIZE=300
            -  "SCHEDULER_NOW="
            -  "SCHEDULER_EXTRA_VARS= -o -w --normal_runtime_seconds 500 --rr_runtime_seconds 30 --save_output -k SCIP"
            -  OBSERVATION_PORTAL_API_TOKEN=sutoken1234abcd
        volumes:
            # Edit these volume maps to wherever you want to store the log files and input/output data sets
            -  ./data/input:/data/adaptive_scheduler/input_states/
            -  ./data/output:/data/adaptive_scheduler/output_schedule/
            -  ./data/:/data/adaptive_scheduler/
            -  ./logs/:/ocs/adaptive_scheduler/logs/
        working_dir: /ocs/adaptive_scheduler/
        entrypoint: /ocs/adaptive_scheduler/docker-entrypoint.sh
        depends_on:
            db:
                condition: service_healthy
            configdb:
                condition: service_healthy

    ocs_frontend:
        image: observatorycontrolsystem/ocs-example-frontend:0.0.1
        ports:
            - "8080:8080"
        restart: always
        environment:
            -  VUE_APP_OBSERVATION_PORTAL_API_URL=http://127.0.0.1:8000
            -  INTERNAL_OBSERVATION_PORTAL_API_URL=http://127.0.0.1:8000
        entrypoint: /entrypoint.sh

    minio1:
        image: minio/minio:RELEASE.2021-06-17T00-10-46Z
        volumes:
            - data1-1:/data1
            - data1-2:/data2
        expose:
            - "9000"
        environment:
            MINIO_ROOT_USER: minio
            MINIO_ROOT_PASSWORD: minio123
            MINIO_REGION_NAME: minioregion
        command: server http://minio{1...4}/data{1...2}
        healthcheck:
            test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
            interval: 30s
            timeout: 20s
            retries: 3
    minio2:
        image: minio/minio:RELEASE.2021-06-17T00-10-46Z
        volumes:
            - data2-1:/data1
            - data2-2:/data2
        expose:
            - "9000"
        environment:
            MINIO_ROOT_USER: minio
            MINIO_ROOT_PASSWORD: minio123
            MINIO_REGION_NAME: minioregion
        command: server http://minio{1...4}/data{1...2}
        healthcheck:
            test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
            interval: 30s
            timeout: 20s
            retries: 3
    minio3:
        image: minio/minio:RELEASE.2021-06-17T00-10-46Z
        volumes:
            - data3-1:/data1
            - data3-2:/data2
        expose:
            - "9000"
        environment:
            MINIO_ROOT_USER: minio
            MINIO_ROOT_PASSWORD: minio123
            MINIO_REGION_NAME: minioregion
        command: server http://minio{1...4}/data{1...2}
        healthcheck:
            test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
            interval: 30s
            timeout: 20s
            retries: 3
    minio4:
        image: minio/minio:RELEASE.2021-06-17T00-10-46Z
        volumes:
            - data4-1:/data1
            - data4-2:/data2
        expose:
            - "9000"
        environment:
            MINIO_ROOT_USER: minio
            MINIO_ROOT_PASSWORD: minio123
            MINIO_REGION_NAME: minioregion
        command: server http://minio{1...4}/data{1...2}
        healthcheck:
            test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
            interval: 30s
            timeout: 20s
            retries: 3

    nginx:
        image: nginx:1.19.2-alpine
        volumes:
        - ./nginx.conf:/etc/nginx/nginx.conf:ro
        network_mode: "service:science_archive"
        depends_on:
        - minio1
        - minio2
        - minio3
        - minio4

    science_archive:
        image: observatorycontrolsystem/science-archive:1.78
        container_name: sciencearchive
        ports:
            - "9000:9000"  # minio
            - "9500:9500"  # archive api
        environment:
            - DEBUG=True  # must be true to load static files
            - DB_HOST=db
            - DB_HOST_READER=db
            - DB_NAME=sciencearchive
            - DB_USER=postgres
            - DB_PASS=postgrespass
            - SECRET_KEY=ocs_example_science_archive_secret_key
            - AWS_ACCESS_KEY_ID=minio
            - AWS_SECRET_ACCESS_KEY=minio123
            - AWS_DEFAULT_REGION=minioregion
            - AWS_BUCKET=ocs-example-bucket
            - S3_ENDPOINT_URL=http://localhost:9000/
            - OAUTH_CLIENT_ID=science_archive_application_client_id
            - OAUTH_CLIENT_SECRET=science_archive_application_client_secret
            - OAUTH_TOKEN_URL=http://observation-portal:8000/o/token/
            - OAUTH_PROFILE_URL=http://observation-portal:8000/api/profile/
            - PROCESSED_EXCHANGE_ENABLED=False
            # Additional variables for the ingester:
            - API_ROOT=http://localhost:9500/
            - BUCKET=ocs-example-bucket
            - OPENTSDB_PYTHON_METRICS_TEST_MODE=True
        mem_limit: "512m"
        restart: always
        volumes: 
            - ./example_data:/example_data
            - ./scripts/science_archive:/scripts
        healthcheck:
            test: ["CMD-SHELL", "wget localhost:9500/ -q -O - > /dev/null 2>&1"]
            interval: 10s
            timeout: 5s
            retries: 5
        command: >
            sh -c "python manage.py migrate ;
            python manage.py collectstatic --no-input ;
            python manage.py runserver 0.0.0.0:9500 &
            pip install ocs_ingester

            export AUTH_TOKEN=$$(\
            echo 'from django.contrib.auth.models import User; 
            User.objects.create_superuser(\"test_user\", \"admin@example.com\", \"test_pass\"); 
            print(User.objects.first().auth_token)' | python manage.py shell
            );

            python /scripts/init_ingester_routine.py ;
            wait %1"
        depends_on:
            minio1:
                condition: service_healthy

    #ingester:
        #image: python:3.9
        #network_mode: "service:science_archive"
        #mem_limit: "512m"
        #volumes:
            #- ./example_data:/example_data
            #- ./scripts/science_archive:/scripts
        #environment:
            #- API_ROOT=http://localhost:9500/
            #- BUCKET=ocs-example-bucket
            #- OPENTSDB_PYTHON_METRICS_TEST_MODE=True
            #- AWS_ACCESS_KEY_ID=minio
            #- AWS_SECRET_ACCESS_KEY=minio123
            #- AWS_DEFAULT_REGION=minioregion
            #- S3_ENDPOINT_URL=http://localhost:9000/
        #command: >
            #sh -c "
            #until $$(curl --output /dev/null --silent --head --fail http://localhost:9500); do sleep 1; done;
            #pip install ocs_ingester boto3;

            #export AUTH_TOKEN=$$(\
            #curl --data \"username=test_user&password=test_pass\" http://localhost:9500/api-token-auth/ | \
            #python3 -c \"import sys, json; print(json.load(sys.stdin)['token'])\"
            #);
            #echo $AUTH_TOKEN

            #python /scripts/init_ingester_routine.py ;
            #"
        #depends_on:
            #- science_archive

    redis:
        image: redis:3.2
        command: ["redis-server", "--appendonly", "yes"]
        restart: always
        ports:
            -  6373:6379
        volumes:
            -  ./data/redis:/data

## By default this config uses default local driver,
## For custom volumes replace with volume driver configuration.
volumes:
    data1-1:
    data1-2:
    data2-1:
    data2-2:
    data3-1:
    data3-2:
    data4-1:
    data4-2:
